From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Johannes Obermayr <johannesobermayr@gmx.de>
Date: Tue, 10 Oct 2023 19:05:17 +0200
Subject: [PATCH] 5872: x86: Simplify MOVSB.REP to memcpy in decompiler

---
 Ghidra/Processors/x86/data/languages/ia.sinc  | 416 ++++++++++++++++--
 .../x86/data/languages/lockable.sinc          |  51 ++-
 2 files changed, 419 insertions(+), 48 deletions(-)

diff --git a/Ghidra/Processors/x86/data/languages/ia.sinc b/Ghidra/Processors/x86/data/languages/ia.sinc
index 1bb4377934..a93c9d74d4 100644
--- a/Ghidra/Processors/x86/data/languages/ia.sinc
+++ b/Ghidra/Processors/x86/data/languages/ia.sinc
@@ -1450,34 +1450,48 @@ moffs64: segWide^[imm32]  is addrsize=1 & highseg=1 & segWide & imm32   { tmp:8
 # TODO: segment register offset in 64bit might not be right
 
 # String memory access
-dseSI1: seg16^SI    is addrsize=0 & seg16 & SI  { tmp:4 = segment(seg16,SI); SI = SI + 1-2*zext(DF); export *:1 tmp; }
-dseSI1: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:4 = ESI; ESI = ESI + 1-2*zext(DF); export *:1 tmp; }
-dseSI2: seg16^SI    is addrsize=0 & seg16 & SI  { tmp:4 = segment(seg16,SI); SI = SI + 2-4*zext(DF); export *:2 tmp; }
-dseSI2: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:4 = ESI; ESI = ESI + 2-4*zext(DF); export *:2 tmp; }
-dseSI4: seg16^SI    is addrsize=0 & seg16 & SI  { tmp:4 = segment(seg16,SI); SI = SI + 4-8*zext(DF); export *:4 tmp; }
-dseSI4: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:4 = ESI; ESI = ESI + 4-8*zext(DF); export *:4 tmp; }
-eseDI1: ES:DI       is addrsize=0 & ES & DI     { tmp:4 = segment(ES,DI); DI = DI + 1-2*zext(DF); export *:1 tmp; }
-eseDI1: ES:EDI      is addrsize=1 & ES & EDI    { tmp:4 = EDI; EDI=EDI+1-2*zext(DF); export *:1 tmp; }
-eseDI2: ES:DI       is addrsize=0 & ES & DI     { tmp:4 = segment(ES,DI); DI = DI + 2-4*zext(DF); export *:2 tmp; }
-eseDI2: ES:EDI      is addrsize=1 & ES & EDI    { tmp:4 = EDI; EDI=EDI+2-4*zext(DF); export *:2 tmp; }
-eseDI4: ES:DI       is addrsize=0 & ES & DI     { tmp:4 = segment(ES,DI); DI = DI + 4-8*zext(DF); export *:4 tmp; }
-eseDI4: ES:EDI      is addrsize=1 & ES & EDI    { tmp:4 = EDI; EDI=EDI+4-8*zext(DF); export *:4 tmp; }
-
 @ifdef IA64
+dseSI1: seg16^SI    is addrsize=0 & seg16 & SI  { tmp1:4 = segment(seg16,SI); tmp:8 = zext(tmp1); export tmp; }
+dseSI1: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:8 = zext(ESI); export tmp; }
+dseSI2: seg16^SI    is addrsize=0 & seg16 & SI  { tmp1:4 = segment(seg16,SI); tmp:8 = zext(tmp1); export tmp; }
+dseSI2: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:8 = zext(ESI); export tmp; }
+dseSI4: seg16^SI    is addrsize=0 & seg16 & SI  { tmp1:4 = segment(seg16,SI); tmp:8 = zext(tmp1); export tmp; }
+dseSI4: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:8 = zext(ESI); export tmp; }
+eseDI1: ES:DI       is addrsize=0 & ES & DI     { tmp1:4 = segment(ES,DI); tmp:8 = zext(tmp1); export tmp; }
+eseDI1: ES:EDI      is addrsize=1 & ES & EDI { tmp:8 = zext(EDI); export tmp; }
+eseDI2: ES:DI       is addrsize=0 & ES & DI     { tmp1:4 = segment(ES,DI); tmp:8 = zext(tmp1); export tmp; }
+eseDI2: ES:EDI      is addrsize=1 & ES & EDI { tmp:8 = zext(EDI); export tmp; }
+eseDI4: ES:DI       is addrsize=0 & ES & DI     { tmp1:4 = segment(ES,DI); tmp:8 = zext(tmp1); export tmp; }
+eseDI4: ES:EDI      is addrsize=1 & ES & EDI { tmp:8 = zext(EDI); export tmp; }
+
 # quadword string functions
-dseSI8: seg16^SI    is addrsize=0 & seg16 & SI  { tmp:4 = segment(seg16,SI); SI = SI + 8-16*zext(DF); export *:8 tmp; }
-dseSI8: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:4 = ESI; ESI = ESI + 8-16*zext(DF); export *:8 tmp; }
-eseDI8: ES:DI       is addrsize=0 & ES & DI     { tmp:4 = segment(ES,DI); DI = DI + 8-16*zext(DF); export *:8 tmp; }
-eseDI8: ES:EDI      is addrsize=1 & ES & EDI    { tmp:4 = EDI; EDI=EDI+8-16*zext(DF); export *:8 tmp; }
+dseSI8: seg16^SI    is addrsize=0 & seg16 & SI  { tmp1:4 = segment(seg16,SI); tmp:8 = zext(tmp1); export tmp; }
+dseSI8: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:8 = zext(ESI); export tmp; }
+eseDI8: ES:DI       is addrsize=0 & ES & DI     { tmp1:4 = segment(ES,DI); tmp:8 = zext(tmp1); export tmp; }
+eseDI8: ES:EDI      is addrsize=1 & ES & EDI { tmp:8 = zext(EDI); export tmp; }
+
+dseSI1: RSI   is addrsize=2 & RSI { export RSI; }
+dseSI2: RSI   is addrsize=2 & RSI { export RSI; }
+dseSI4: RSI   is addrsize=2 & RSI { export RSI; }
+dseSI8: RSI   is addrsize=2 & RSI { export RSI; }
+eseDI1: RDI   is addrsize=2 & RDI { export RDI; }
+eseDI2: RDI   is addrsize=2 & RDI { export RDI; }
+eseDI4: RDI   is addrsize=2 & RDI { export RDI; }
+eseDI8: RDI   is addrsize=2 & RDI { export RDI; }
 
-dseSI1: RSI   is addrsize=2 & RSI { local tmp = RSI; RSI = RSI + 1-2*zext(DF); export *:1 tmp; }
-dseSI2: RSI   is addrsize=2 & RSI { local tmp = RSI; RSI = RSI + 2-4*zext(DF); export *:2 tmp; }
-dseSI4: RSI   is addrsize=2 & RSI { local tmp = RSI; RSI = RSI + 4-8*zext(DF); export *:4 tmp; }
-dseSI8: RSI   is addrsize=2 & RSI { local tmp = RSI; RSI = RSI + 8-16*zext(DF); export *:8 tmp; }
-eseDI1: RDI   is addrsize=2 & RDI    { local tmp = RDI; RDI=RDI+1-2*zext(DF); export *:1 tmp; }
-eseDI2: RDI   is addrsize=2 & RDI    { local tmp = RDI; RDI=RDI+2-4*zext(DF); export *:2 tmp; }
-eseDI4: RDI   is addrsize=2 & RDI    { local tmp = RDI; RDI=RDI+4-8*zext(DF); export *:4 tmp; }
-eseDI8: RDI   is addrsize=2 & RDI    { local tmp = RDI; RDI=RDI+8-16*zext(DF); export *:8 tmp; }
+@else
+dseSI1: seg16^SI    is addrsize=0 & seg16 & SI  { tmp:4 = segment(seg16,SI); export tmp; }
+dseSI1: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:4 = ESI; export tmp; }
+dseSI2: seg16^SI    is addrsize=0 & seg16 & SI  { tmp:4 = segment(seg16,SI); export tmp; }
+dseSI2: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:4 = ESI; export tmp; }
+dseSI4: seg16^SI    is addrsize=0 & seg16 & SI  { tmp:4 = segment(seg16,SI); export tmp; }
+dseSI4: segWide^ESI   is addrsize=1 & segWide & ESI { tmp:4 = ESI; export tmp; }
+eseDI1: ES:DI       is addrsize=0 & ES & DI     { tmp:4 = segment(ES,DI); export tmp; }
+eseDI1: ES:EDI      is addrsize=1 & ES & EDI { tmp:4 = EDI; export tmp; }
+eseDI2: ES:DI       is addrsize=0 & ES & DI     { tmp:4 = segment(ES,DI); export tmp; }
+eseDI2: ES:EDI      is addrsize=1 & ES & EDI { tmp:4 = EDI; export tmp; }
+eseDI4: ES:DI       is addrsize=0 & ES & DI     { tmp:4 = segment(ES,DI); export tmp; }
+eseDI4: ES:EDI      is addrsize=1 & ES & EDI { tmp:4 = EDI; export tmp; }
 @endif
 
 rm8: Rmr8   is mod=3 & Rmr8     { export Rmr8; }
@@ -1552,6 +1566,8 @@ rep: ".REP" is ((repprefx=1 & repneprefx=0)|(repprefx=0 & repneprefx=1)) & addrs
 rep: ".REP" is ((repprefx=1 & repneprefx=0)|(repprefx=0 & repneprefx=1)) & addrsize=2  { if (RCX==0) goto inst_next; RCX=RCX-1; }
 @endif
 rep:        is repprefx=0 & repneprefx=0			{ }
+repx: ".REP" is ((repprefx=1 & repneprefx=0)|(repprefx=0 & repneprefx=1)) { }
+repx:        is repprefx=0 & repneprefx=0			{ }
 
 reptail:	is ((repprefx=1 & repneprefx=0)|(repprefx=0 & repneprefx=1))			{ goto inst_start; }
 reptail:	is repprefx=0 & repneprefx=0			{ }
@@ -2351,6 +2367,38 @@ macro fucompe(val1, val2) {
     SF = 0;
 }
 
+macro newpos(pos1, pos2, off) {
+    if (DF == 1) goto <sub>;
+@ifdef IA64
+    pos1 = pos1 + zext(off);
+    pos2 = pos2 + zext(off);
+    goto <end>;
+<sub>
+    pos1 = pos1 - zext(off);
+    pos2 = pos2 - zext(off);
+@else
+    pos1 = pos1 + off;
+    pos2 = pos2 + off;
+    goto <end>;
+<sub>
+    pos1 = pos1 - off;
+    pos2 = pos2 - off;
+@endif
+<end>
+}
+
+define pcodeop memcmp;
+macro do_memcmp(reg1, reg2, size, count) {
+    total:$(SIZE) = size:$(SIZE) * count:$(SIZE);
+    if (DF == 1) goto <sub>;
+    ZF = memcmp(reg1, reg2, total);
+    goto <end>;
+<sub>
+    ZF = memcmp(reg1-total, reg2-total, total);
+<end>
+}
+
+
 # The base level constructors
 #   The prefixes
 :^instruction is instrPhase=0 & over=0x2e; instruction     [ segover=1; ]  {} # CS override
@@ -3085,11 +3133,51 @@ define pcodeop clzero;
 :CMP Reg64,rm64     is $(LONGMODE_ON) & vexMode=0 & opsize=2 & byte=0x3b; rm64 & Reg64 ...       { local temp:8 = rm64; subflags(Reg64,temp); local diff = Reg64 - temp; resultflags(diff); }
 @endif
 
-:CMPSB^repe^repetail eseDI1,dseSI1  is vexMode=0 & repe & repetail & byte=0xa6 & dseSI1 & eseDI1        { build repe; build eseDI1; build dseSI1; local temp_DI1:1 = eseDI1; local temp_SI1:1 = dseSI1;  subflags(temp_SI1,temp_DI1); local diff=temp_SI1 - temp_DI1; resultflags(diff); build repetail; }
-:CMPSW^repe^repetail eseDI2,dseSI2  is vexMode=0 & repe & repetail & opsize=0 & byte=0xa7 & dseSI2 & eseDI2 { build repe; build eseDI2; build dseSI2; local temp_DI2:2 = eseDI2; local temp_SI2:2 = dseSI2; subflags(temp_SI2,temp_DI2); local diff=temp_SI2 - temp_DI2; resultflags(diff); build repetail; }
-:CMPSD^repe^repetail eseDI4,dseSI4  is vexMode=0 & repe & repetail & opsize=1 & byte=0xa7 & dseSI4 & eseDI4 { build repe; build eseDI4; build dseSI4; local temp_DI4:4 = eseDI4; local temp_SI4:4 = dseSI4; subflags(temp_SI4,temp_DI4); local diff=temp_SI4 - temp_DI4; resultflags(diff); build repetail; }
+:CMPSB^repx eseDI1,dseSI1  is vexMode=0 & repx & byte=0xa6 & dseSI1 & eseDI1
+{
+    tmp:$(SIZE) = 1;
+    do_memcmp(eseDI1,dseSI1,tmp,CX);
+    newpos(DI,SI,CX);
+}
+:CMPSB^repx eseDI2,dseSI2  is vexMode=0 & addrsize=0 & repx & byte=0xa6 & dseSI2 & eseDI2
+{
+    tmp:$(SIZE) = 1;
+    do_memcmp(eseDI2,dseSI2,tmp,CX);
+    newpos(DI,SI,CX);
+}
+:CMPSB^repx eseDI4,dseSI4  is vexMode=0 & addrsize=1 & repx & byte=0xa6 & dseSI4 & eseDI4
+{
+    tmp:$(SIZE) = 1;
+    do_memcmp(eseDI4,dseSI4,tmp,ECX);
+    newpos(EDI,ESI,ECX);
+}
+@ifdef IA64
+:CMPSB^repx eseDI8,dseSI8  is vexMode=0 & addrsize=2 & repx & byte=0xa6 & eseDI8 & dseSI8
+{
+    tmp:$(SIZE) = 1;
+    do_memcmp(eseDI8,dseSI8,tmp,RCX);
+    newpos(RDI,RSI,RCX);
+}
+@endif
+:CMPSW^repx eseDI2,dseSI2  is vexMode=0 & addrsize=0 & repx & opsize=0 & byte=0xa7 & dseSI2 & eseDI2
+{
+    tmp:$(SIZE) = 2;
+    do_memcmp(eseDI2,dseSI2,tmp,CX);
+    newpos(DI,SI,CX*2);
+}
+:CMPSD^repx eseDI4,dseSI4  is vexMode=0 & addrsize=1 & repx & opsize=0 & byte=0xa7 & dseSI4 & eseDI4
+{
+    tmp:$(SIZE) = 4;
+    do_memcmp(eseDI4,dseSI4,tmp,ECX);
+    newpos(EDI,ESI,ECX*4);
+}
 @ifdef IA64
-:CMPSD^repe^repetail eseDI8,dseSI8  is $(LONGMODE_ON) & vexMode=0 & repe & repetail & opsize=2 & byte=0xa7 & dseSI8 & eseDI8 { build repe; build eseDI8; build dseSI8; local temp_DI8:8 = eseDI8; local temp_SI8:8 = dseSI8; subflags(temp_SI8,temp_DI8); local diff=temp_SI8-temp_DI8; resultflags(diff); build repetail; }
+:CMPSD^repx eseDI8,dseSI8  is vexMode=0 & addrsize=2 & repx & opsize=0 & byte=0xa7 & dseSI8 & eseDI8
+{
+    tmp:$(SIZE) = 4;
+    do_memcmp(eseDI8,dseSI8,tmp,RCX);
+    newpos(RDI,RSI,RCX*4);
+}
 @endif
 
 # See 'lockable.sinc' for memory destination, lockable variants
@@ -4192,11 +4280,35 @@ define pcodeop LocalDescriptorTableRegister;
 }
 @endif
 
-:LODSB^rep^reptail dseSI1   is vexMode=0 & rep & reptail & byte=0xAC & dseSI1           { build rep; build dseSI1; AL=dseSI1; build reptail; }
-:LODSW^rep^reptail dseSI2   is vexMode=0 & rep & reptail & opsize=0 & byte=0xAD & dseSI2    { build rep; build dseSI2; AX=dseSI2; build reptail; }
-:LODSD^rep^reptail dseSI4   is vexMode=0 & rep & reptail & opsize=1 & byte=0xAD & dseSI4    { build rep; build dseSI4; EAX=dseSI4; build reptail; }
+define pcodeop memcpy;
+:LODSB^repx dseSI1	is vexMode=0 & repx & repprefx & byte=0xAC & dseSI1
+{
+  memcpy(AL, dseSI1, CX);
+  AX = AX + CX;
+  SI = SI + CX;
+}
+:LODSW^repx dseSI2	is vexMode=0 & repx & repprefx & opsize=0 & byte=0xAD & dseSI2
+{
+  local tmp = CX * 2;
+  memcpy(AX, dseSI2, tmp);
+  AX = AX + tmp;
+  SI = SI + tmp;
+}
+:LODSD^repx dseSI4	is vexMode=0 & repx & repprefx & opsize=1 & byte=0xAD & dseSI4
+{
+  local tmp = ECX * 4;
+  memcpy(EAX, dseSI4, tmp);
+  EAX = EAX + tmp;
+  ESI = ESI + tmp;
+}
 @ifdef IA64
-:LODSQ^rep^reptail dseSI8   is $(LONGMODE_ON) & vexMode=0 & rep & reptail & opsize=2 & byte=0xAD & dseSI8    { build rep; build dseSI8; RAX=dseSI8; build reptail; }
+:LODSQ^repx dseSI8	is $(LONGMODE_ON) & vexMode=0 & repx & repprefx & opsize=2 & byte=0xAD & dseSI8
+{
+  local tmp = RCX * 8;
+  memcpy(RAX, dseSI8, tmp);
+  RAX = RAX + tmp;
+  RSI = RSI + tmp;
+}
 @endif
 
 :LOOP   rel8        is vexMode=0 & addrsize=0 & byte=0xE2; rel8             { CX = CX -1; if (CX!=0) goto rel8; }
@@ -4414,11 +4526,135 @@ define pcodeop swap_bytes;
 :MOVNTI Mem,Reg64       is $(LONGMODE_ON) & vexMode=0 & opsize = 2; byte=0xf; byte=0xc3; Mem & Reg64 ...     { *Mem = Reg64; }
 @endif
 
-:MOVSB^rep^reptail eseDI1,dseSI1    is vexMode=0 & rep & reptail & byte=0xa4 & eseDI1 & dseSI1          { build rep; build eseDI1; build dseSI1; eseDI1 = dseSI1; build reptail; }
-:MOVSW^rep^reptail eseDI2,dseSI2    is vexMode=0 & rep & reptail & opsize=0 & byte=0xa5 & eseDI2 & dseSI2   { build rep; build eseDI2; build dseSI2; eseDI2 = dseSI2; build reptail; }
-:MOVSD^rep^reptail eseDI4,dseSI4    is vexMode=0 & rep & reptail & opsize=1 & byte=0xa5 & eseDI4 & dseSI4   { build rep; build eseDI4; build dseSI4; eseDI4 = dseSI4; build reptail; }
+define pcodeop memmove;
+:MOVSB^repx eseDI1,dseSI1	is vexMode=0 & repx & repprefx & byte=0xa4 & eseDI1 & dseSI1
+{
+  local tmp = CX;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 1;
+<code>
+  memmove(eseDI1, dseSI1, CX);
+  DI = DI + tmp;
+  SI = SI + tmp;
+}
+:MOVSB^repx eseDI2,dseSI2	is vexMode=0 & addrsize=0 & repx & repprefx & byte=0xa4 & eseDI2 & dseSI2
+{
+  local tmp = CX;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 1;
+<code>
+  memmove(eseDI2, dseSI2, tmp);
+  DI = DI + tmp;
+  SI = SI + tmp;
+}
+:MOVSB^repx eseDI4,dseSI4	is vexMode=0 & addrsize=1 & repx & repprefx & byte=0xa4 & eseDI4 & dseSI4
+{
+  local tmp = ECX;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 1;
+<code>
+  memmove(eseDI4, dseSI4, tmp);
+  EDI = EDI + tmp;
+  ESI = ESI + tmp;
+}
+@ifdef IA64
+:MOVSB^repx RDI,RSI	is vexMode=0 & addrsize=2 & repx & repprefx & byte=0xa4 & RDI & RSI
+{
+  local tmp = ECX;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 1;
+<code>
+  memmove(RDI, RSI, tmp);
+  RDI = RDI + zext(tmp);
+  RSI = RSI + zext(tmp);
+}
+@endif
+
+:MOVSW^repx eseDI2,dseSI2	is vexMode=0 & addrsize=0 & repx & repprefx & opsize=0 & byte=0xa5 & eseDI2 & dseSI2
+{
+  local tmp = CX * 2;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 2;
+<code>
+  memmove(eseDI2, dseSI2, tmp);
+  DI = DI + tmp;
+  SI = SI + tmp;
+}
+:MOVSW^repx eseDI4,dseSI4	is vexMode=0 & addrsize=1 & repx & repprefx & opsize=0 & byte=0xa5 & eseDI4 & dseSI4
+{
+
+  local tmp = ECX * 2;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 2;
+<code>
+  memmove(eseDI4, dseSI4, tmp);
+  EDI = EDI + tmp;
+  ESI = ESI + tmp;
+}
 @ifdef IA64
-:MOVSQ^rep^reptail eseDI8,dseSI8    is $(LONGMODE_ON) & vexMode=0 & rep & reptail & opsize=2 & byte=0xa5 & eseDI8 & dseSI8   { build rep; build eseDI8; build dseSI8; eseDI8 = dseSI8; build reptail; }
+:MOVSW^repx RDI,RSI	is vexMode=0 & addrsize=2 & repx & repprefx & opsize=0 & byte=0xa5 & RDI & RSI
+{
+  local tmp:8 = zext(ECX) * 2;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 2;
+<code>
+  memmove(RDI, RSI, tmp);
+  RDI = RDI + tmp;
+  RSI = RSI + tmp;
+}
+@endif
+:MOVSD^repx eseDI2,dseSI2	is vexMode=0 & addrsize=0 & repx & repprefx & opsize=1 & byte=0xa5 & eseDI2 & dseSI2
+{
+  local tmp = CX * 4;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 4;
+<code>
+  memmove(eseDI2, dseSI2, tmp);
+  DI = DI + tmp;
+  SI = SI + tmp;
+}
+:MOVSD^repx eseDI4,dseSI4	is vexMode=0 & addrsize=1 & repx & repprefx & opsize=1 & byte=0xa5 & eseDI4 & dseSI4
+{
+  local tmp = ECX * 4;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 4;
+<code>
+  memmove(eseDI4, dseSI4, tmp);
+  EDI = EDI + tmp;
+  ESI = ESI + tmp;
+}
+@ifdef IA64
+:MOVSD^repx RDI,RSI	is vexMode=0 & addrsize=2 & repx & repprefx & opsize=1 & byte=0xa5 & RDI & RSI
+{
+  local tmp:8 = zext(ECX) * 4;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 4;
+<code>
+  memmove(RDI, RSI, tmp);
+  RDI = RDI + tmp;
+  RSI = RSI + tmp;
+}
+:MOVSQ^repx RDI,RSI	is $(LONGMODE_ON) & vexMode=0 & addrsize=2 & repx & repprefx & opsize=2 & byte=0xa5 & RDI & RSI
+{
+  local tmp:8 = zext(ECX) * 8;
+  repprefx_:1 = repprefx;
+  if (repprefx_ == 1) goto <code>;
+  tmp = 8;
+<code>
+  memmove(RDI, RSI, tmp);
+  RDI = RDI + tmp;
+  RSI = RSI + tmp;
+}
 @endif
 
 :MOVSX Reg16,rm8    is vexMode=0 & opsize=0 & byte=0xf; byte=0xbe; rm8 & Reg16 ...  { Reg16 = sext(rm8); }
@@ -4917,11 +5153,64 @@ define pcodeop smm_restore_state;
 :SBB  Reg64,rm64       is $(LONGMODE_ON) & vexMode=0 & opsize=2 & byte=0x1b; rm64 & Reg64 ...						{ subCarryFlags( Reg64, rm64 ); resultflags(Reg64); }
 @endif
 
-:SCASB^repe^repetail eseDI1 is vexMode=0 & repe & repetail &            byte=0xae & eseDI1  { build repe; build eseDI1; subflags(AL,eseDI1); local diff=AL-eseDI1; resultflags(diff); build repetail; }
-:SCASW^repe^repetail eseDI2 is vexMode=0 & repe & repetail & opsize=0 & byte=0xaf & eseDI2  { build repe; build eseDI2; subflags(AX,eseDI2); local diff=AX-eseDI2; resultflags(diff); build repetail; }
-:SCASD^repe^repetail eseDI4 is vexMode=0 & repe & repetail & opsize=1 & byte=0xaf & eseDI4  { build repe; build eseDI4; subflags(EAX,eseDI4); local diff=EAX-eseDI4; resultflags(diff); build repetail; }
+define pcodeop strchr;
+:SCASB^repe eseDI1 is vexMode=0 & repe & byte=0xae & eseDI1
+{
+  CX = strchr(eseDI1, AX);
+  DI = DI + AX;
+}
+:SCASB^repe eseDI2 is vexMode=0 & addrsize=0 & repe & byte=0xae & eseDI2
+{
+  CX = strchr(eseDI2, AX);
+  DI = DI + AX;
+}
+:SCASB^repe eseDI4 is vexMode=0 & addrsize=1 & repe & byte=0xae & eseDI4
+{
+  ECX = strchr(eseDI4, EAX);
+  EDI = EDI + EAX;
+}
+@ifdef IA64
+:SCASB^repe eseDI8 is vexMode=0 & addrsize=2 & repe & byte=0xae & eseDI8
+{
+  RCX = strchr(eseDI8, RAX);
+  RDI = RDI + RAX;
+}
+@endif
+
+:SCASW^repe eseDI2 is vexMode=0 & addrsize=0 & repe & opsize=0 & byte=0xaf & eseDI2
+{
+  CX = strchr(eseDI2, AX);
+  DI = DI + AX * 2;
+}
+:SCASW^repe eseDI4 is vexMode=0 & addrsize=1 & repe & opsize=0 & byte=0xaf & eseDI4
+{
+  ECX = strchr(eseDI4, EAX);
+  EDI = EDI + EAX * 2;
+}
+@ifdef IA64
+:SCASW^repe eseDI8 is vexMode=0 & addrsize=2 & repe & opsize=0 & byte=0xaf & eseDI8
+{
+  RCX = strchr(eseDI8, RAX);
+  RDI = RDI + RAX * 2;
+}
+@endif
+
+:SCASD^repe eseDI4 is vexMode=0 & addrsize=1 & repe & opsize=1 & byte=0xaf & eseDI4
+{
+  ECX = strchr(eseDI4, EAX);
+  EDI = EDI + EAX * 4;
+}
 @ifdef IA64
-:SCASQ^repe^repetail eseDI8 is $(LONGMODE_ON) & vexMode=0 & repe & repetail & opsize=2 & byte=0xaf & eseDI8  { build repe; build eseDI8; subflags(RAX,eseDI8); local diff=RAX-eseDI8; resultflags(diff); build repetail; }
+:SCASD^repe eseDI8 is vexMode=0 & addrsize=2 & repe & opsize=1 & byte=0xaf & eseDI8
+{
+  RCX = strchr(eseDI8, RAX);
+  RDI = RDI + RAX * 4;
+}
+:SCASQ^repe eseDI8 is $(LONGMODE_ON) & vexMode=0 & repe & opsize=2 & byte=0xaf & eseDI8
+{
+  RCX = strchr(eseDI8, RAX);
+  RDI = RDI + RAX * 8;
+}
 @endif
 
 :SET^cc rm8     is vexMode=0 & byte=0xf; row=9 & cc; rm8                { rm8 = cc; }
@@ -5085,11 +5374,48 @@ define pcodeop stac;
 
 :STMXCSR m32        is vexMode=0 & byte=0xf; byte=0xae; ( mod != 0b11 & reg_opcode=3 ) ... & m32 { m32 = MXCSR; }
 
-:STOSB^rep^reptail eseDI1   is vexMode=0 & rep & reptail & byte=0xaa & eseDI1           { build rep; build eseDI1; eseDI1=AL; build reptail; }
-:STOSW^rep^reptail eseDI2   is vexMode=0 & rep & reptail & opsize=0 & byte=0xab & eseDI2    { build rep; build eseDI2; eseDI2=AX; build reptail; }
-:STOSD^rep^reptail eseDI4   is vexMode=0 & rep & reptail & opsize=1 & byte=0xab & eseDI4    { build rep; build eseDI4; eseDI4=EAX; build reptail; }
+define pcodeop memset;
+:STOSB^repx eseDI1,AL	is vexMode=0 & repx & byte=0xaa & eseDI1 & AL
+{
+  memset(eseDI1, AL, CX);
+  DI = DI + CX;
+}
+:STOSB^repx eseDI2,AX	is vexMode=0 & addrsize=0 & repx & byte=0xaa & eseDI2 & AX
+{
+  memset(eseDI2, AX, CX);
+  DI = DI + CX;
+}
+:STOSB^repx eseDI4,EAX	is vexMode=0 & addrsize=1 & repx & byte=0xaa & eseDI4 & EAX
+{
+  memset(eseDI4, EAX, ECX);
+  EDI = EDI + ECX;
+}
 @ifdef IA64
-:STOSQ^rep^reptail eseDI8   is $(LONGMODE_ON) & vexMode=0 & rep & reptail & opsize=2 & byte=0xab & eseDI8    { build rep; build eseDI8; eseDI8=RAX; build reptail; }
+:STOSB^repx eseDI8,RAX   is vexMode=0 & addrsize=2 & repx & byte=0xaa & eseDI8 & RAX
+{
+  memset(RDI, RAX, ECX);
+  RDI = RDI + RCX;
+}
+@endif
+:STOSW^repx eseDI2,AX   is vexMode=0 & repx & opsize=0 & byte=0xab & eseDI2 & AX
+{
+  local tmp = CX * 2;
+  memset(eseDI2, AX, tmp);
+  DI = DI + tmp;
+}
+:STOSD^repx eseDI4,EAX   is vexMode=0 & repx & opsize=1 & byte=0xab & eseDI4 & EAX
+{
+  local tmp = ECX * 4;
+  memset(eseDI4, EAX, tmp);
+  EDI = EDI + tmp;
+}
+@ifdef IA64
+:STOSQ^repx eseDI8,RAX   is $(LONGMODE_ON) & vexMode=0 & repx & opsize=2 & byte=0xab & eseDI8 & RAX
+{
+  local tmp = RCX * 8;
+  memset(eseDI8, RAX, tmp);
+  RDI = RDI + tmp;
+}
 @endif
 
 :STR rm16       is vexMode=0 & byte=0xf; byte=0x0; rm16 & reg_opcode=1 ... { rm16 = TaskRegister(); }
diff --git a/Ghidra/Processors/x86/data/languages/lockable.sinc b/Ghidra/Processors/x86/data/languages/lockable.sinc
index 52a28a2ff0..5a66ac5947 100644
--- a/Ghidra/Processors/x86/data/languages/lockable.sinc
+++ b/Ghidra/Processors/x86/data/languages/lockable.sinc
@@ -735,6 +735,14 @@ define pcodeop InterlockedIncrement64;
 }
 @endif
 
+define pcodeop InterlockedExchange;
+define pcodeop InterlockedExchange8;
+define pcodeop InterlockedExchange16;
+:MOV^lockx_ CRmr8,imm8     is vexMode=0 & row=11 & lockx_ & lockprefx & page=0 & CRmr8; imm8
+{
+    InterlockedExchange8(CRmr8:1, imm8:1);
+}
+
 :NEG^lockx m8       is vexMode=0 & lockx & unlock & byte=0xf6; m8 & reg_opcode=3 ...         
 {
     build lockx;
@@ -1033,6 +1041,46 @@ define pcodeop InterlockedIncrement64;
 }
 @endif
 
+:SBB^lockx  Reg8,rm8        is vexMode=0 & lockx & unlock & byte=0x1a; rm8 & Reg8 ...
+{
+    build lockx;
+    build Reg8;
+    subCarryFlags( Reg8, rm8 );
+    resultflags(Reg8);
+    build unlock;
+}
+
+:SBB^lockx  Reg16,rm16       is vexMode=0 & lockx & unlock & opsize=0 & byte=0x1b; rm16 & Reg16 ...
+{
+    build lockx;
+    build Reg16;
+    subCarryFlags( Reg16, rm16 );
+    resultflags(Reg16);
+    build unlock;
+}
+
+:SBB^lockx  Reg32,rm32       is vexMode=0 & lockx & unlock & opsize=1 & byte=0x1b; rm32 & Reg32 ... & check_Reg32_dest ...
+{
+    build lockx;
+    build Reg32;
+    subCarryFlags( Reg32, rm32 );
+    build check_Reg32_dest;
+    resultflags(Reg32);
+    build unlock;
+}
+
+@ifdef IA64
+:SBB^lockx  Reg64,rm64       is $(LONGMODE_ON) & vexMode=0 & lockx & unlock & opsize=2 & byte=0x1b; rm64 & Reg64 ...
+{
+    build lockx;
+    build Reg64;
+    subCarryFlags( Reg64, rm64 );
+    resultflags(Reg64);
+    build unlock;
+}
+@endif
+
+
 :SUB^lockx  m8,imm8     is vexMode=0 & lockx & unlock & $(BYTE_80_82); m8 & reg_opcode=5 ...; imm8     
 {
     build lockx;
@@ -1173,9 +1221,6 @@ define pcodeop InterlockedExchangeAdd64;
 }
 @endif
 
-define pcodeop InterlockedExchange;
-define pcodeop InterlockedExchange8;
-define pcodeop InterlockedExchange16;
 # XCHG with memory operands always asserts a lock signal regardless of prefix presence
 :XCHG^xacq_xrel_prefx^alwaysLock Mem,Reg8	is vexMode=0 & xacq_xrel_prefx & alwaysLock & byte=0x86; Mem & Reg8 ...
 {
-- 
2.45.1

