From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Sleigh-InSPECtor <sleighinspector@outlook.com>
Date: Thu, 18 Apr 2024 23:37:25 +0930
Subject: [PATCH] 6442: AArch64: Change eor3 OR to XOR

AA64: Change eor3 OR to XOR
---
 .../AARCH64/data/languages/AARCH64neon.sinc   | 64 +++++++++----------
 1 file changed, 32 insertions(+), 32 deletions(-)

diff --git a/Ghidra/Processors/AARCH64/data/languages/AARCH64neon.sinc b/Ghidra/Processors/AARCH64/data/languages/AARCH64neon.sinc
index f063193923..d68a6c28c0 100644
--- a/Ghidra/Processors/AARCH64/data/languages/AARCH64neon.sinc
+++ b/Ghidra/Processors/AARCH64/data/languages/AARCH64neon.sinc
@@ -3465,39 +3465,39 @@ is b_3131=0 & q=0 & u=1 & b_2428=0xe & advSIMD3.size=0 & b_2121=1 & Rm_VPR64.8B
 is b_2131=0b11001110000 & b_15=0 & Rd_VPR128.16B & Rn_VPR128.16B & Rm_VPR128.16B & Ra_VPR128.16B & Zd
 {
 	# simd infix TMPQ1 = Rm_VPR128.16B | Ra_VPR128.16B on lane size 1
-	TMPQ1[0,8] = Rm_VPR128.16B[0,8] | Ra_VPR128.16B[0,8];
-	TMPQ1[8,8] = Rm_VPR128.16B[8,8] | Ra_VPR128.16B[8,8];
-	TMPQ1[16,8] = Rm_VPR128.16B[16,8] | Ra_VPR128.16B[16,8];
-	TMPQ1[24,8] = Rm_VPR128.16B[24,8] | Ra_VPR128.16B[24,8];
-	TMPQ1[32,8] = Rm_VPR128.16B[32,8] | Ra_VPR128.16B[32,8];
-	TMPQ1[40,8] = Rm_VPR128.16B[40,8] | Ra_VPR128.16B[40,8];
-	TMPQ1[48,8] = Rm_VPR128.16B[48,8] | Ra_VPR128.16B[48,8];
-	TMPQ1[56,8] = Rm_VPR128.16B[56,8] | Ra_VPR128.16B[56,8];
-	TMPQ1[64,8] = Rm_VPR128.16B[64,8] | Ra_VPR128.16B[64,8];
-	TMPQ1[72,8] = Rm_VPR128.16B[72,8] | Ra_VPR128.16B[72,8];
-	TMPQ1[80,8] = Rm_VPR128.16B[80,8] | Ra_VPR128.16B[80,8];
-	TMPQ1[88,8] = Rm_VPR128.16B[88,8] | Ra_VPR128.16B[88,8];
-	TMPQ1[96,8] = Rm_VPR128.16B[96,8] | Ra_VPR128.16B[96,8];
-	TMPQ1[104,8] = Rm_VPR128.16B[104,8] | Ra_VPR128.16B[104,8];
-	TMPQ1[112,8] = Rm_VPR128.16B[112,8] | Ra_VPR128.16B[112,8];
-	TMPQ1[120,8] = Rm_VPR128.16B[120,8] | Ra_VPR128.16B[120,8];
+	TMPQ1[0,8] = Rm_VPR128.16B[0,8] ^ Ra_VPR128.16B[0,8];
+	TMPQ1[8,8] = Rm_VPR128.16B[8,8] ^ Ra_VPR128.16B[8,8];
+	TMPQ1[16,8] = Rm_VPR128.16B[16,8] ^ Ra_VPR128.16B[16,8];
+	TMPQ1[24,8] = Rm_VPR128.16B[24,8] ^ Ra_VPR128.16B[24,8];
+	TMPQ1[32,8] = Rm_VPR128.16B[32,8] ^ Ra_VPR128.16B[32,8];
+	TMPQ1[40,8] = Rm_VPR128.16B[40,8] ^ Ra_VPR128.16B[40,8];
+	TMPQ1[48,8] = Rm_VPR128.16B[48,8] ^ Ra_VPR128.16B[48,8];
+	TMPQ1[56,8] = Rm_VPR128.16B[56,8] ^ Ra_VPR128.16B[56,8];
+	TMPQ1[64,8] = Rm_VPR128.16B[64,8] ^ Ra_VPR128.16B[64,8];
+	TMPQ1[72,8] = Rm_VPR128.16B[72,8] ^ Ra_VPR128.16B[72,8];
+	TMPQ1[80,8] = Rm_VPR128.16B[80,8] ^ Ra_VPR128.16B[80,8];
+	TMPQ1[88,8] = Rm_VPR128.16B[88,8] ^ Ra_VPR128.16B[88,8];
+	TMPQ1[96,8] = Rm_VPR128.16B[96,8] ^ Ra_VPR128.16B[96,8];
+	TMPQ1[104,8] = Rm_VPR128.16B[104,8] ^ Ra_VPR128.16B[104,8];
+	TMPQ1[112,8] = Rm_VPR128.16B[112,8] ^ Ra_VPR128.16B[112,8];
+	TMPQ1[120,8] = Rm_VPR128.16B[120,8] ^ Ra_VPR128.16B[120,8];
 	# simd infix Rd_VPR128.16B = Rn_VPR128.16B | TMPQ1 on lane size 1
-	Rd_VPR128.16B[0,8] = Rn_VPR128.16B[0,8] | TMPQ1[0,8];
-	Rd_VPR128.16B[8,8] = Rn_VPR128.16B[8,8] | TMPQ1[8,8];
-	Rd_VPR128.16B[16,8] = Rn_VPR128.16B[16,8] | TMPQ1[16,8];
-	Rd_VPR128.16B[24,8] = Rn_VPR128.16B[24,8] | TMPQ1[24,8];
-	Rd_VPR128.16B[32,8] = Rn_VPR128.16B[32,8] | TMPQ1[32,8];
-	Rd_VPR128.16B[40,8] = Rn_VPR128.16B[40,8] | TMPQ1[40,8];
-	Rd_VPR128.16B[48,8] = Rn_VPR128.16B[48,8] | TMPQ1[48,8];
-	Rd_VPR128.16B[56,8] = Rn_VPR128.16B[56,8] | TMPQ1[56,8];
-	Rd_VPR128.16B[64,8] = Rn_VPR128.16B[64,8] | TMPQ1[64,8];
-	Rd_VPR128.16B[72,8] = Rn_VPR128.16B[72,8] | TMPQ1[72,8];
-	Rd_VPR128.16B[80,8] = Rn_VPR128.16B[80,8] | TMPQ1[80,8];
-	Rd_VPR128.16B[88,8] = Rn_VPR128.16B[88,8] | TMPQ1[88,8];
-	Rd_VPR128.16B[96,8] = Rn_VPR128.16B[96,8] | TMPQ1[96,8];
-	Rd_VPR128.16B[104,8] = Rn_VPR128.16B[104,8] | TMPQ1[104,8];
-	Rd_VPR128.16B[112,8] = Rn_VPR128.16B[112,8] | TMPQ1[112,8];
-	Rd_VPR128.16B[120,8] = Rn_VPR128.16B[120,8] | TMPQ1[120,8];
+	Rd_VPR128.16B[0,8] = Rn_VPR128.16B[0,8] ^ TMPQ1[0,8];
+	Rd_VPR128.16B[8,8] = Rn_VPR128.16B[8,8] ^ TMPQ1[8,8];
+	Rd_VPR128.16B[16,8] = Rn_VPR128.16B[16,8] ^ TMPQ1[16,8];
+	Rd_VPR128.16B[24,8] = Rn_VPR128.16B[24,8] ^ TMPQ1[24,8];
+	Rd_VPR128.16B[32,8] = Rn_VPR128.16B[32,8] ^ TMPQ1[32,8];
+	Rd_VPR128.16B[40,8] = Rn_VPR128.16B[40,8] ^ TMPQ1[40,8];
+	Rd_VPR128.16B[48,8] = Rn_VPR128.16B[48,8] ^ TMPQ1[48,8];
+	Rd_VPR128.16B[56,8] = Rn_VPR128.16B[56,8] ^ TMPQ1[56,8];
+	Rd_VPR128.16B[64,8] = Rn_VPR128.16B[64,8] ^ TMPQ1[64,8];
+	Rd_VPR128.16B[72,8] = Rn_VPR128.16B[72,8] ^ TMPQ1[72,8];
+	Rd_VPR128.16B[80,8] = Rn_VPR128.16B[80,8] ^ TMPQ1[80,8];
+	Rd_VPR128.16B[88,8] = Rn_VPR128.16B[88,8] ^ TMPQ1[88,8];
+	Rd_VPR128.16B[96,8] = Rn_VPR128.16B[96,8] ^ TMPQ1[96,8];
+	Rd_VPR128.16B[104,8] = Rn_VPR128.16B[104,8] ^ TMPQ1[104,8];
+	Rd_VPR128.16B[112,8] = Rn_VPR128.16B[112,8] ^ TMPQ1[112,8];
+	Rd_VPR128.16B[120,8] = Rn_VPR128.16B[120,8] ^ TMPQ1[120,8];
 	zext_zq(Zd); # zero upper 16 bytes of Zd
 }
 
-- 
2.45.1

